import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import FlowShrink.utils as utils
import FlowShrink.shortest_paths_gpu as ws

import torch
import torch._dynamo
torch._dynamo.config.suppress_errors = True # é¿å…ä¸€äº›æ— å…³è­¦å‘Š
import numpy as np
from scipy.sparse import coo_matrix
import time

class MCNFPDHG:
    def __init__(self,dtype=torch.float64):
        self.device = torch.device('cuda:0')
        self.dtype = dtype
    
    def create_data(self, num_nodes, k, num_commodities, seed=1, warm_start=False):
        self.N = num_nodes
        self.K = num_commodities
        device = self.device
        dtype=self.dtype
        if dtype==torch.float64:
            npdtype=np.float64
        else:
            npdtype=np.float32

        # adjacency and incidence
        W_adj = utils.create_base_network(self.N, k, seed)
        W_adj = utils.ensure_weak_connectivity(W_adj, seed)
        A_inc_np, p_np = utils.adjacency_to_incidence(W_adj)# (N, M)
        commodities = utils.create_commodities(W_adj, self.K, 10.0, seed)
        if warm_start:
            self.W_adj=torch.tensor(W_adj,dtype=dtype,device=device)
        else:
            self.W_adj=None
        del W_adj
        
        # capacities
        c_np = utils.generate_capacity_constraints(A_inc_np, commodities, 1.0, 5.0, seed=seed)
        self.c = torch.from_numpy(c_np.astype(npdtype)).to(self.device)
        A_inc=torch.from_numpy(A_inc_np)
        del A_inc_np
        '''
        torch.where(condition) åœ¨å¤„ç†äºŒç»´å¼ é‡æ—¶ï¼Œè¿”å›çš„ç´¢å¼•æ˜¯æŒ‰ç…§ Row-Majorï¼ˆè¡Œä¼˜å…ˆï¼‰ é¡ºåºæ’åˆ—çš„ï¼Œå³å…ˆæ‰«æç¬¬0è¡Œï¼Œå†æ‰«æç¬¬1è¡Œï¼Œä»¥æ­¤ç±»æ¨ã€‚
        ç„¶è€Œï¼Œä½ çš„ cï¼ˆå®¹é‡ï¼‰ã€pï¼ˆè´¹ç”¨ï¼‰ä»¥åŠå˜é‡ x éƒ½æ˜¯æŒ‰ç…§ Edge Indexï¼ˆåˆ—ç´¢å¼• 0 åˆ° M-1ï¼‰ æ’åˆ—çš„ã€‚
        ä½ çš„å‡è®¾ï¼šedges_src[j] å¯¹åº”ç¬¬ j æ¡è¾¹ï¼ˆå³ A_inc çš„ç¬¬ j åˆ—ï¼‰çš„æºèŠ‚ç‚¹ã€‚
        å®é™…æƒ…å†µï¼šedges_src åªæ˜¯åŒ…å«äº†æ‰€æœ‰æºèŠ‚ç‚¹çš„åˆ—è¡¨ï¼Œä½†æŒ‰ç…§èŠ‚ç‚¹IDæ’åºï¼ˆå—è¡Œæ‰«æé¡ºåºå½±å“ï¼‰ï¼Œå®Œå…¨æ‰“ä¹±äº†ä¸è¾¹ç´¢å¼• 0...M-1 çš„å¯¹åº”å…³ç³»ã€‚ 
        å®é™…å˜æˆäº†æ²¿ç€dim=1å¯»æ‰¾ 
        åæœï¼š
        PDHG æ±‚è§£å™¨å®é™…ä¸Šæ˜¯åœ¨ä¸€ä¸ª ä¹±è¿çº¿çš„å›¾ ä¸Šè¿›è¡Œä¼˜åŒ–ã€‚è¾¹çš„èµ·ç‚¹å’Œç»ˆç‚¹è¢«é‡æ–°æ´—ç‰Œäº†ï¼Œä½†è¾¹çš„å®¹é‡å’Œè´¹ç”¨å´ä¿æŒåŸåºã€‚     
        '''
        # self.edges_src=torch.where(A_inc==-1)[0].to(device)# M
        # self.edges_dst=torch.where(A_inc==1)[0].to(device)# M
        
        # argmin æ‰¾åˆ°æ¯åˆ—æœ€å°å€¼çš„ç´¢å¼•ï¼ˆå³ -1 æ‰€åœ¨çš„è¡Œç´¢å¼•ï¼‰
        self.edges_src = torch.argmin(A_inc, dim=0).to(device)
        # argmax æ‰¾åˆ°æ¯åˆ—æœ€å¤§å€¼çš„ç´¢å¼•ï¼ˆå³ 1 æ‰€åœ¨çš„è¡Œç´¢å¼•ï¼‰
        self.edges_dst = torch.argmax(A_inc, dim=0).to(device)
        self.M=A_inc.shape[1]
        del A_inc

        # edge cost
        p = torch.from_numpy(p_np.astype(npdtype)).to(self.device)
        del p_np

        # W, d
        W_scale=300.0
        #self.Wå·²ç»ä¹˜äº†W_scale
        self.W = torch.from_numpy(utils.generate_weight(self.K,dimtype='vector', seed=seed)*W_scale).to(self.device).to(dtype)
        commodity_src = [c[0] for c in commodities]
        commodity_dst = [c[1] for c in commodities]
        demands = [c[2] for c in commodities]
        self.d = torch.tensor(demands, dtype=dtype, device=self.device)
        # tensors used as indices must be long, int, byte or bool tensors
        self.k_src=torch.tensor(commodity_src, dtype=torch.long, device=self.device)
        self.k_dst=torch.tensor(commodity_dst, dtype=torch.long, device=self.device)
        del commodity_src,commodity_dst,demands,W_scale

        # keep p (M) on device
        self.p = p

        # f_mat (N,K) small-ish dense (-1,0,1)
        f_list = []
        for kk in range(self.K):
            f_np = np.zeros(self.N, dtype=npdtype)
            s_idx, t_idx = commodities[kk][0], commodities[kk][1]
            f_np[s_idx] = -1.0
            f_np[t_idx] = 1.0
            f_list.append(torch.from_numpy(f_np))
        self.f_mat = torch.stack(f_list, dim=1).to(self.device)        

        return self.N, self.M
    
    def pdhg_step_fn(self,x_prev, X_prev, Y, x_bar,X_bar, sigma, tau,  
                 K, M, overrelax_rho):
        # dual update,explicit,prox is here
        Y_new = Y + sigma * (self.A_matvec(x_bar) - self.S_matvec(X_bar))

        #primal update
        v=(x_prev-tau*self.AT_matvec(Y_new)).reshape(M,K)-tau*self.p.unsqueeze(1)
        #x update as projection
        x_new= self.proj(v,self.c)
        #X update as proximal operator
        X_new = self.f1_prox(X_prev + tau * self.ST_matvec(Y_new), tau)
        
        #overrelaxation
        x_bar=(1+overrelax_rho)*x_new-overrelax_rho*x_prev
        X_bar=(1+overrelax_rho)*X_new-overrelax_rho*X_prev
    
        return x_new, X_new, Y_new, x_bar, X_bar


    # -------------------------
    # çŸ©é˜µ-å‘é‡æ¥å£ï¼ˆç¨€ç–åŒ–ï¼‰
    # -------------------------
    def A_matvec(self, x):
        flow = x.view(self.M, self.K)
        # åˆå§‹åŒ–ç»“æœ (N, K)
        div = torch.zeros((self.N, self.K), device=x.device, dtype=x.dtype)
        div.index_add_(0, self.edges_dst, flow)
        div.index_add_(0, self.edges_src, -flow)
        
        return div.reshape(self.N * self.K)

    def AT_matvec(self, y):
        potentials = y.view(self.N, self.K)
        # tension: (M, K)
        tension = potentials[self.edges_dst] - potentials[self.edges_src]
        
        return tension.reshape(self.M * self.K)

    def S_matvec(self, X):
        K, N = self.K, self.N
        #pytorchçš„å¹¿æ’­æœºåˆ¶ç”¨äºæ ‡é‡*å‘é‡æ—¶ï¼š
        #f_matä¸ºNÃ—KçŸ©é˜µï¼Œæ­¤å¤„æ“ä½œåº”ä¸ºå¯¹f_matçš„æ¯ä¸€åˆ—ï¼Œç”¨æ ‡é‡X_kå»ä¹˜
        #æ­£ç¡®æ–¹æ³•åº”ä¸ºå°†Xå¹¿æ’­ä¸º1è¡ŒKåˆ—ï¼Œå¯¹åº”f_matçš„Kåˆ—ï¼Œæ¯ä¸€åˆ—ä¸€ä¸ªæ ‡é‡ä¸è¯¥åˆ—åšä¹˜æ³•ï¼ˆåˆ—çº¿æ€§å˜æ¢ï¼‰ï¼Œå³X_col = X.unsqueeze(0)
        #æˆ–ç›´æ¥çœç•¥unsqueezeï¼Œè¿ç®—ç¬¦*ä¼šè§¦å‘pytorchçš„è‡ªåŠ¨æ ‡é‡ä¹˜å¹¿æ’­
        #æ­¤å¤„X_col = X.unsqueeze(1)æ²¡æœ‰æŠ¥é”™ï¼Œæ˜¯å› ä¸ºæµ‹è¯•æ•°æ®ä¸­N==Kï¼Œæ©ç›–äº†ç»´åº¦çš„ä¸åŒ¹é…
        blocks = self.f_mat * X   # N x K
        return blocks.reshape(N * K)

    def ST_matvec(self, Y):
        K, N = self.K, self.N
        Y_mat = Y.view(N, K)
        return torch.sum(self.f_mat * Y_mat, dim=0)
    
    def power_iteration_K_norm(self, iters=50):
        """
        Compute ||K||_2 where K = [A  -S].
        A here is mathcal(A), the linear operator in PDHG, not the adjacent or incidence matrix of the graph
        """
        dtype=self.dtype
        device = self.device
        u = torch.randn(self.K*self.M, device=device, dtype=dtype)#MK
        v=torch.randn(self.K, device=device, dtype=dtype)#K
        uv=torch.cat([u,v],dim=0)
        uv=uv/uv.norm()#MK+K
        
        for _ in range(iters):
            u,v=torch.split(uv,self.K*self.M)
            # K [u; v] = ğ“(u) - S(v),KN
            Kuv = self.A_matvec(u) - self.S_matvec(v)

            # Káµ€(K[u;v])
            # Káµ€ y = [ğ“áµ€ y ; -Sáµ€ y]
            KT_K_u = self.AT_matvec(Kuv)#KM=KM*KN * KN
            KT_K_v = -self.ST_matvec(Kuv)#K=K*KN * KN

            uv_next = torch.cat([KT_K_u, KT_K_v], dim=0)
            norm_next = uv_next.norm()
            uv = uv_next / norm_next#(M+1)*K

        # sqrt of eigenvalue of Káµ€K
        K_norm = norm_next.sqrt()
        return K_norm


    # -------------------------
    # PDHG solver with automated tau/sigma tuning and relaxation theta
    # -------------------------
    def pdhg_solve(self,
                x0=None, X0=None,
                tau=None, sigma=None,
                kappa_Y=1.0,
                max_iter=100000, tol=1e-2,
                verbose=True, overrelax_rho=1.0, check_interval=500):
        dev = self.device
        K, M = self.K, self.M
        dtype = self.dtype

        if x0 is None:
            x = torch.zeros(M*K, device=dev, dtype=dtype)
        else:
            x = x0.clone().to(dev)
        if X0 is None:
            X = torch.zeros(K, device=dev, dtype=dtype)
        else:
            X = X0.clone().to(dev)

        # ensure data on device
        self.p = self.p.to(dev)
        self.c = self.c.to(dev)
        self.d = self.d.to(dev)
        self.W = self.W.to(dev)
        self.f_mat = self.f_mat.to(dev)
           
        # calculate the 2-norm of linear operator in our problem formulation to ensure convergence
        K_norm = self.power_iteration_K_norm()
        eta = 0.9 / K_norm # safer estimation, K_norm is derived by iteration
        pweight = torch.tensor(1.0)
        tau = eta/pweight
        sigma = eta*pweight

        # residual-based dual init
        rY = self.A_matvec(x) - self.S_matvec(X)
        Y = -kappa_Y * rY
        x_prev = x.clone()
        X_prev = X.clone()
        x_bar = x.clone()
        X_bar = X.clone()

        if dev.type == 'cuda':
            torch.cuda.synchronize()
        t0 = time.time()

        for it in range(max_iter):
            x_new, X_new, Y_new, x_bar, X_bar = self.pdhg_step_fn(x_prev,X_prev,Y,x_bar,X_bar,sigma,tau,self.K,self.M,overrelax_rho)
            
            if (it == max_iter - 1):
                print(f'Max iterations reached, r_p={r_primal:.2e}, r_d={r_dual:.2e}')

            if it % check_interval == 0:
                # residuals
                with torch.no_grad():
                    r_primal = torch.norm(self.A_matvec(x_bar) - self.S_matvec(X_bar))
                    r_dual = torch.norm(x_new - x_prev)/tau + torch.norm(X_new - X_prev)/tau
                    tau, sigma, pweight = self.weight_update(r_primal,r_dual,pweight,eta,tau)
                    rp_val=r_primal.item()
                    rd_val=r_dual.item()

                
                if verbose:
                    print(f'Iter {it:6d} | r_p={r_primal:.2e} | r_d={r_dual:.2e} | pweight={pweight}')
                    
                if (rp_val < tol) and (rd_val < tol):
                    print(f'Converged at iter {it}, r_p={r_primal:.2e}, r_d={r_dual:.2e}')
                    break
            
            #shift iteration
            x_prev,X_prev=x_new,X_new
            Y=Y_new

        if dev.type == 'cuda':
            torch.cuda.synchronize()
        if verbose:
            print('PDHG total time:', time.time()-t0)
        return x_new, X_new, Y_new

    def weight_update(self, r_primal,r_dual,pweight, eta, tau):
        scaling = torch.tensor(0.5, device=self.device) # theta
        ratio = r_primal / (r_dual + 1e-12)
        log_p=torch.log(pweight)
        cond1=ratio>10.0
        cond2=ratio<0.1
        change = torch.where(cond1, scaling, torch.where(cond2, -scaling, torch.tensor(0.0, device=self.device)))
        pweight_new=torch.exp(log_p + change)
        pweight_new = torch.clamp(pweight_new, 1e-5, 1e5)
        tau = eta / pweight_new
        sigma = eta * pweight_new
        return tau, sigma, pweight_new
    
    # -------------------------
    # prox functions
    # -------------------------
    def f1_prox(self, X_tilde, tau):
        return (X_tilde+2.0*tau*self.W*self.d) / (1.0+2.0*tau*self.W)
    
    
    def proj(self, U, c):
        """
        U: (M, K) unconstrained flow M ROW K COL
        c: (M,) flow capacity per edge
        """
        c_expanded = c.unsqueeze(1) # (M, 1)
        
        # 1. Clip negative values
        U_clipped = torch.clamp(U, min=0)
        
        # 2. Check sum constraint along dim=1 (Commodities)
        row_sum = U_clipped.sum(dim=1, keepdim=True) # (M, 1)
        
        # 3. Sort along dim=1
        U_sorted, _ = torch.sort(U_clipped, dim=1, descending=True)
        
        # 4. Cumsum along dim=1
        S_cum = U_sorted.cumsum(dim=1)
        
        # 5. Calculate Tau Candidates
        # (M, K) - (M, 1) / (1, K) -> (M, K)
        tau_candidates = (S_cum - c_expanded) / torch.arange(1,self.K+1,device= U.device,dtype=U.dtype).view(1,self.K)
        
        # 6. Find rho (active set size)
        cond = U_sorted > tau_candidates
        # Count true values along dim=1
        rho = cond.type(torch.int8).sum(dim=1) - 1
        rho = torch.clamp(rho, min=0) # (M,)
        
        # 7. Gather Tau
        # rho shape (M,), need (M, 1) to gather from (M, K)
        tau_selected = torch.gather(tau_candidates, 1, rho.unsqueeze(1)) # (M, 1)
        
        # 8. Projection
        x_proj = torch.clamp(U_clipped - tau_selected, min=0)
        
        # 9. Final Select
        # If sum <= c, keep original, else project
        need_proj = row_sum > c_expanded
        x_out = torch.where(need_proj, x_proj, U_clipped)
        
        return x_out.reshape(self.M*self.K)

    def make_initials(self):
        dtype=self.dtype
        dev = self.device
        if self.W_adj is None:
            x0 = torch.zeros(self.M * self.K, device=dev, dtype=dtype)
            X0 = torch.zeros(self.K, device=dev, dtype=dtype)
        else:
            x0,X0=self.generate_initial_flow_gpu()
        return x0, X0
    
    def generate_initial_flow_gpu(self):
        """
        åœ¨ GPU ä¸Šæ ¹æ®å‰é©±ï¼ˆNext-Hopï¼‰çŸ©é˜µ P é‡å»ºè·¯å¾„å¹¶ç”Ÿæˆåˆå§‹æµ x0ã€‚
            
        è¿”å›:
            x0: torch.Tensor, (M * K), å±•å¹³çš„åˆå§‹æµé‡å‘é‡
            X0: torch.Tensor, (K,), åˆå§‹é€è¾¾é‡å‘é‡, ä¸éœ€æ±‚ç›¸åŒ
        """
        device=self.device
        dtype=self.dtype
        # 1. æ•°æ®å‡†å¤‡
        K = self.K
        # å„ä¸ªcommoditiesçš„åŸç‚¹å’Œæ±‡ç‚¹
        k_src = self.k_src
        k_dst = self.k_dst
        demands = self.d # (K,)
        edges_src=self.edges_src
        edges_dst=self.edges_dst
        M=self.M
        N=self.N
        _,P,_=ws.apsp_gpu(self.W_adj,dtype=dtype)
        del self.W_adj
        self.W_adj=None

        # 2. æ„å»º (u, v) -> edge_index çš„å¿«é€ŸæŸ¥æ‰¾è¡¨
        # è¿™ä¸€æ­¥åªéœ€è¦åšä¸€æ¬¡ã€‚å¦‚æœæ˜¯ç±»æˆå‘˜å‡½æ•°ï¼Œå¯ä»¥åœ¨ __init__ æˆ– create_data ä¸­ç¼“å­˜ edge_lookup
        edge_lookup = torch.full((N, N), -1, dtype=torch.long, device=device)
        edge_lookup[edges_src, edges_dst] = torch.arange(M, device=device)

        # 3. åˆå§‹åŒ–æµé‡çŸ©é˜µ (M, K)
        x_flow = torch.zeros((M, K), dtype=dtype, device=device)
        
        # 4. å¹¶è¡Œè·¯å¾„è¿½è¸ª (Pointer Chasing)
        # æ‰€æœ‰å•†å“å¹¶è¡Œä» s å‡ºå‘èµ°å‘ t
        curr_nodes = k_src.clone()
        
        # è®°å½•å“ªäº›å•†å“å·²ç»åˆ°è¾¾ç»ˆç‚¹ï¼Œé¿å…å¤šä½™è®¡ç®—
        active_mask = torch.ones(K, dtype=torch.bool, device=device)
        
        # å¾ªç¯æ¬¡æ•°ä¸Šé™è®¾ä¸º N (æœ€åæƒ…å†µè·¯å¾„é•¿åº¦)
        # å®é™…ä¸Šå¯¹äºç¨€ç–å›¾å’Œå°ç›´å¾„ç½‘ç»œï¼Œè¿™ä¸ªå¾ªç¯ä¼šéå¸¸å¿«
        for _ in range(N):
            # å¦‚æœæ‰€æœ‰å•†å“éƒ½åˆ°è¾¾ç»ˆç‚¹ï¼Œæå‰é€€å‡º
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ’é™¤æ‰é‚£äº› s==t çš„çç¢æƒ…å†µï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            arrived = (curr_nodes == k_dst)
            active_mask = active_mask & (~arrived)
            
            if not active_mask.any():
                break
                
            # --- æ ¸å¿ƒé€»è¾‘ ---
            
            # 1. æŸ¥æ‰¾ä¸‹ä¸€è·³èŠ‚ç‚¹
            # P shape [N, N]. gather indices: curr_nodes [K], t_indices [K]
            # P[u, v] ä»£è¡¨ä» u å» v çš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
            # åˆ©ç”¨ Advanced Indexing: P[row_indices, col_indices]
            next_nodes = P[curr_nodes, k_dst] # shape (K,)
            
            # 2. æŸ¥æ‰¾å¯¹åº”çš„è¾¹ç´¢å¼•
            edge_ids = edge_lookup[curr_nodes, next_nodes] # shape (K,)
            
            # 3. åªæœ‰ active ä¸” è¾¹å­˜åœ¨çš„å•†å“æ‰æ›´æ–°æµé‡
            # edge_ids == -1 è¯´æ˜å›¾ä¸è¿é€šæˆ– P çŸ©é˜µæŒ‡å¼•äº†ä¸å­˜åœ¨çš„è¾¹
            valid_step = active_mask & (edge_ids != -1)
            
            if not valid_step.any():
                # æ‰€æœ‰æ´»è·ƒçš„å•†å“éƒ½æ‰¾ä¸åˆ°è·¯äº†ï¼ˆå›¾ä¸è¿é€šï¼‰ï¼Œç›´æ¥é€€å‡ºé˜²æ­¢æ­»å¾ªç¯
                break
                
            # 4. å¡«å…¥æµé‡
            # é€‰å–æœ‰æ•ˆçš„ k ç´¢å¼•
            valid_k = torch.nonzero(valid_step, as_tuple=True)[0]
            valid_e = edge_ids[valid_step]
            
            x_flow[valid_e, valid_k] = demands[valid_step]
            
            # 5. æ›´æ–°ä½ç½®
            curr_nodes[valid_step] = next_nodes[valid_step]

            # 5. å±•å¹³å¹¶è¿”å› (M*K)å’Œdemandï¼ˆæ¾å¼›å®¹é‡çº¦æŸï¼Œåˆ™ä¸€å®šå…¨éƒ¨é€è¾¾ï¼ŒX==dï¼‰
        return x_flow.reshape(-1),self.d
