import os
import sys

import torch
import torch._dynamo
import numpy as np
from scipy.sparse import coo_matrix
import time

import FlowShrink.utils as utils
import FlowShrink.shortest_paths_gpu as ws

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
torch._dynamo.config.suppress_errors = True  # type: ignore # é¿å…ä¸€äº›æ— å…³è­¦å‘Š


class MCNFPDHGWARMSTART:
    def __init__(self, dtype=torch.float64):
        self.device = torch.device("cuda:0")
        self.dtype = dtype
        self.weight_update_scaling=torch.tensor(0.5,device=self.device)
        self.best_rp=torch.tensor(torch.inf,device=self.device)
        self.best_rd=torch.tensor(torch.inf,device=self.device)
        

    def create_data(self, num_nodes, k, num_commodities, seed=1):
        self.N = num_nodes
        self.K = num_commodities
        device = self.device
        dtype = self.dtype
        if dtype == torch.float64:
            npdtype = np.float64
        else:
            npdtype = np.float32

        # adjacency and incidence
        W_adj = utils.create_base_network(self.N, k, seed)
        W_adj = utils.ensure_weak_connectivity(W_adj, seed)
        A_inc_np, p_np = utils.adjacency_to_incidence(W_adj)  # (N, M)
        commodities = utils.create_commodities(W_adj, self.K, 10.0, seed)
        self.W_adj = torch.tensor(W_adj, dtype=dtype, device=device)
        del W_adj

        # capacities
        c_np = utils.generate_capacity_constraints(
            A_inc_np, commodities, 1.0, 5.0, seed=seed
        )
        self.c = torch.from_numpy(c_np.astype(npdtype)).to(self.device)
        A_inc = torch.from_numpy(A_inc_np)
        del A_inc_np
        """
        torch.where(condition) åœ¨å¤„ç†äºŒç»´å¼ é‡æ—¶ï¼Œè¿”å›çš„ç´¢å¼•æ˜¯æŒ‰ç…§ Row-Majorï¼ˆè¡Œä¼˜å…ˆï¼‰ é¡ºåºæ’åˆ—çš„ï¼Œå³å…ˆæ‰«æç¬¬0è¡Œï¼Œå†æ‰«æç¬¬1è¡Œï¼Œä»¥æ­¤ç±»æ¨ã€‚
        ç„¶è€Œï¼Œä½ çš„ cï¼ˆå®¹é‡ï¼‰ã€pï¼ˆè´¹ç”¨ï¼‰ä»¥åŠå˜é‡ x éƒ½æ˜¯æŒ‰ç…§ Edge Indexï¼ˆåˆ—ç´¢å¼• 0 åˆ° M-1ï¼‰ æ’åˆ—çš„ã€‚
        ä½ çš„å‡è®¾ï¼šedges_src[j] å¯¹åº”ç¬¬ j æ¡è¾¹ï¼ˆå³ A_inc çš„ç¬¬ j åˆ—ï¼‰çš„æºèŠ‚ç‚¹ã€‚
        å®é™…æƒ…å†µï¼šedges_src åªæ˜¯åŒ…å«äº†æ‰€æœ‰æºèŠ‚ç‚¹çš„åˆ—è¡¨ï¼Œä½†æŒ‰ç…§èŠ‚ç‚¹IDæ’åºï¼ˆå—è¡Œæ‰«æé¡ºåºå½±å“ï¼‰ï¼Œå®Œå…¨æ‰“ä¹±äº†ä¸è¾¹ç´¢å¼• 0...M-1 çš„å¯¹åº”å…³ç³»ã€‚ 
        å®é™…å˜æˆäº†æ²¿ç€dim=1å¯»æ‰¾ 
        åæœï¼š
        PDHG æ±‚è§£å™¨å®é™…ä¸Šæ˜¯åœ¨ä¸€ä¸ª ä¹±è¿çº¿çš„å›¾ ä¸Šè¿›è¡Œä¼˜åŒ–ã€‚è¾¹çš„èµ·ç‚¹å’Œç»ˆç‚¹è¢«é‡æ–°æ´—ç‰Œäº†ï¼Œä½†è¾¹çš„å®¹é‡å’Œè´¹ç”¨å´ä¿æŒåŸåºã€‚     
        """
        # self.edges_src=torch.where(A_inc==-1)[0].to(device)# M
        # self.edges_dst=torch.where(A_inc==1)[0].to(device)# M

        # argmin æ‰¾åˆ°æ¯åˆ—æœ€å°å€¼çš„ç´¢å¼•ï¼ˆå³ -1 æ‰€åœ¨çš„è¡Œç´¢å¼•ï¼‰
        self.edges_src = torch.argmin(A_inc, dim=0).to(device)
        # argmax æ‰¾åˆ°æ¯åˆ—æœ€å¤§å€¼çš„ç´¢å¼•ï¼ˆå³ 1 æ‰€åœ¨çš„è¡Œç´¢å¼•ï¼‰
        self.edges_dst = torch.argmax(A_inc, dim=0).to(device)
        self.M = A_inc.shape[1]
        del A_inc

        # edge cost
        p = torch.from_numpy(p_np.astype(npdtype)).to(self.device)
        del p_np

        # W, d
        W_scale = 300.0
        # self.Wå·²ç»ä¹˜äº†W_scale
        self.W = (
            torch.from_numpy(
                utils.generate_weight(self.K, dimtype="vector", seed=seed) * W_scale
            )
            .to(self.device)
            .to(dtype)
        )
        commodity_src = [c[0] for c in commodities]
        commodity_dst = [c[1] for c in commodities]
        demands = [c[2] for c in commodities]
        self.d = torch.tensor(demands, dtype=dtype, device=self.device)
        # tensors used as indices must be long, int, byte or bool tensors
        self.k_src = torch.tensor(commodity_src, dtype=torch.long, device=self.device)
        self.k_dst = torch.tensor(commodity_dst, dtype=torch.long, device=self.device)
        del commodity_src, commodity_dst, demands, W_scale

        # keep p (M) on device
        self.p = p

        # f_mat (N,K) small-ish dense (-1,0,1)
        f_list = []
        for kk in range(self.K):
            f_np = np.zeros(self.N, dtype=npdtype)
            s_idx, t_idx = commodities[kk][0], commodities[kk][1]
            f_np[s_idx] = -1.0
            f_np[t_idx] = 1.0
            f_list.append(torch.from_numpy(f_np))
        self.f_mat = torch.stack(f_list, dim=1).to(self.device)

        return self.N, self.M

    def pdhg_step_fn(
        self, x_prev, X_prev, Y, x_bar, X_bar, sigma, tau, K, M, overrelax_rho
    ):
        # dual update,explicit,prox is here
        Y_new = Y + sigma * (self.A_matvec(x_bar) - self.S_matvec(X_bar))

        # primal update
        v = (x_prev - tau * self.AT_matvec(Y_new)).reshape(
            M, K
        ) - tau * self.p.unsqueeze(1)
        # x update as projection
        x_new = self.proj(v, self.c)
        # X update as proximal operator
        X_new = self.f1_prox(X_prev + tau * self.ST_matvec(Y_new), tau)

        # overrelaxation
        x_bar = (1 + overrelax_rho) * x_new - overrelax_rho * x_prev
        X_bar = (1 + overrelax_rho) * X_new - overrelax_rho * X_prev

        return x_new, X_new, Y_new, x_bar, X_bar

    # -------------------------
    # çŸ©é˜µ-å‘é‡æ¥å£ï¼ˆç¨€ç–åŒ–ï¼‰
    # -------------------------
    def A_matvec(self, x):
        flow = x.view(self.M, self.K)
        # åˆå§‹åŒ–ç»“æœ (N, K)
        div = torch.zeros((self.N, self.K), device=x.device, dtype=x.dtype)
        div.index_add_(0, self.edges_dst, flow)
        div.index_add_(0, self.edges_src, -flow)

        return div.reshape(self.N * self.K)

    def AT_matvec(self, y):
        potentials = y.view(self.N, self.K)
        # tension: (M, K)
        tension = potentials[self.edges_dst] - potentials[self.edges_src]

        return tension.reshape(self.M * self.K)

    def S_matvec(self, X):
        K, N = self.K, self.N
        # pytorchçš„å¹¿æ’­æœºåˆ¶ç”¨äºæ ‡é‡*å‘é‡æ—¶ï¼š
        # f_matä¸ºNÃ—KçŸ©é˜µï¼Œæ­¤å¤„æ“ä½œåº”ä¸ºå¯¹f_matçš„æ¯ä¸€åˆ—ï¼Œç”¨æ ‡é‡X_kå»ä¹˜
        # æ­£ç¡®æ–¹æ³•åº”ä¸ºå°†Xå¹¿æ’­ä¸º1è¡ŒKåˆ—ï¼Œå¯¹åº”f_matçš„Kåˆ—ï¼Œæ¯ä¸€åˆ—ä¸€ä¸ªæ ‡é‡ä¸è¯¥åˆ—åšä¹˜æ³•ï¼ˆåˆ—çº¿æ€§å˜æ¢ï¼‰ï¼Œå³X_col = X.unsqueeze(0)
        # æˆ–ç›´æ¥çœç•¥unsqueezeï¼Œè¿ç®—ç¬¦*ä¼šè§¦å‘pytorchçš„è‡ªåŠ¨æ ‡é‡ä¹˜å¹¿æ’­
        # æ­¤å¤„X_col = X.unsqueeze(1)æ²¡æœ‰æŠ¥é”™ï¼Œæ˜¯å› ä¸ºæµ‹è¯•æ•°æ®ä¸­N==Kï¼Œæ©ç›–äº†ç»´åº¦çš„ä¸åŒ¹é…
        blocks = self.f_mat * X  # N x K
        return blocks.reshape(N * K)

    def ST_matvec(self, Y):
        K, N = self.K, self.N
        Y_mat = Y.view(N, K)
        return torch.sum(self.f_mat * Y_mat, dim=0)

    def power_iteration_K_norm(self, iters=50):
        """
        Compute ||K||_2 where K = [A  -S].
        A here is mathcal(A), the linear operator in PDHG, not the adjacent or incidence matrix of the graph
        """
        dtype = self.dtype
        device = self.device
        u = torch.randn(self.K * self.M, device=device, dtype=dtype)  # MK
        v = torch.randn(self.K, device=device, dtype=dtype)  # K
        uv = torch.cat([u, v], dim=0)
        uv = uv / uv.norm()  # MK+K

        for _ in range(iters):
            u, v = torch.split(uv, self.K * self.M)
            # K [u; v] = ğ“(u) - S(v),KN
            Kuv = self.A_matvec(u) - self.S_matvec(v)

            # Káµ€(K[u;v])
            # Káµ€ y = [ğ“áµ€ y ; -Sáµ€ y]
            KT_K_u = self.AT_matvec(Kuv)  # KM=KM*KN * KN
            KT_K_v = -self.ST_matvec(Kuv)  # K=K*KN * KN

            uv_next = torch.cat([KT_K_u, KT_K_v], dim=0)
            norm_next = uv_next.norm()
            uv = uv_next / norm_next  # (M+1)*K

        # sqrt of eigenvalue of Káµ€K
        K_norm = norm_next.sqrt()
        return K_norm

    # -------------------------
    # PDHG solver with automated tau/sigma tuning and relaxation theta
    # -------------------------
    def pdhg_solve(
        self,
        x0,
        X0,
        Y0,
        tau=None,
        sigma=None,
        max_iter=200000,
        tol=1e-2,
        verbose=True,
        overrelax_rho=1.0,
        check_interval=500,
    ):
        dev = self.device
        x = x0.to(dev)
        X = X0.to(dev)
        
        # ensure data on device
        self.p = self.p.to(dev)
        self.c = self.c.to(dev)
        self.d = self.d.to(dev)
        self.W = self.W.to(dev)
        self.f_mat = self.f_mat.to(dev)

        # calculate the 2-norm of linear operator in our problem formulation to ensure convergence
        K_norm = self.power_iteration_K_norm()
        eta = 0.9 / K_norm  # safer estimation, K_norm is derived by iteration
        pweight = torch.tensor(1.0)
        tau = eta / pweight
        sigma = eta * pweight

        Y = Y0.to(dev).clone()
        x_prev = x.clone()
        X_prev = X.clone()
        x_bar = x.clone()
        X_bar = X.clone()

        if dev.type == "cuda":
            torch.cuda.synchronize()
        t0 = time.time()

        for it in range(max_iter):
            x_new, X_new, Y_new, x_bar, X_bar = self.pdhg_step_fn(
                x_prev,
                X_prev,
                Y,
                x_bar,
                X_bar,
                sigma,
                tau,
                self.K,
                self.M,
                overrelax_rho,
            )

            if it == max_iter - 1:
                print(f"Max iterations reached, r_p={r_primal:.2e}, r_d={r_dual:.2e}")  # noqa: F821

            if it % check_interval == 0:
                # residuals
                with torch.no_grad():
                    r_primal = torch.norm(self.A_matvec(x_bar) - self.S_matvec(X_bar))
                    r_dual = (
                        torch.norm(x_new - x_prev) / tau
                        + torch.norm(X_new - X_prev) / tau
                    )
                    tau, sigma, pweight = self.weight_update(
                        r_primal, r_dual, pweight, eta, tau
                    )
                    rp_val = r_primal.item()
                    rd_val = r_dual.item()

                if verbose:
                    print(
                        f"Iter {it:6d} | r_p={r_primal:.2e} | r_d={r_dual:.2e} | pweight={pweight}"
                    )

                if (rp_val < tol) and (rd_val < tol):
                    print(
                        f"Converged at iter {it}, r_p={r_primal:.2e}, r_d={r_dual:.2e}"
                    )
                    break

            # shift iteration
            x_prev, X_prev = x_new, X_new
            Y = Y_new

        if dev.type == "cuda":
            torch.cuda.synchronize()
        if verbose:
            print("PDHG total time:", time.time() - t0)
        return x_new, X_new, Y_new


    def weight_update(self, r_primal, r_dual, pweight, eta, tau):
        # cond_rp=r_primal>self.best_rp
        # cond_rd=r_dual>self.best_rd
        # cond_residual=cond_rp | cond_rd
        # eta_new=torch.where(cond_residual,eta*0.9,eta)
        # self.best_rp=torch.where(cond_rp,self.best_rp,r_primal)
        # self.best_rd=torch.where(cond_rd,self.best_rd,r_dual)
        scaling = self.weight_update_scaling  # theta
        ratio = r_primal / (r_dual + 1e-12)
        log_p = torch.log(pweight)
        cond1 = ratio > 2.0
        cond2 = ratio < 0.5
        change = torch.where(
            cond1,
            scaling,
            torch.where(cond2, -scaling, torch.tensor(0.0, device=self.device)),
        )
        pweight_new = torch.exp(log_p + change)
        pweight_new = torch.clamp(pweight_new, 1e-5, 1e5)
        tau = eta / pweight_new
        sigma = eta * pweight_new
        return tau, sigma, pweight_new#,eta_new

    # -------------------------
    # prox functions
    # -------------------------
    def f1_prox(self, X_tilde, tau):
        return (X_tilde + 2.0 * tau * self.W * self.d) / (1.0 + 2.0 * tau * self.W)

    def proj(self, U, c):
        """
        Active Set ç­–ç•¥ï¼šåªå¯¹è¿åå®¹é‡çº¦æŸçš„è¡Œè¿›è¡Œ Sort å’Œç²¾ç¡®æŠ•å½±ã€‚
        ä¿æŒäº†æ•°å­¦çš„ç²¾ç¡®æ€§ï¼ˆæ”¶æ•›å¿«ï¼‰ï¼ŒåŒæ—¶é¿å…äº†å¤§é‡çš„æ— æ•ˆè®¡ç®—ï¼ˆé€Ÿåº¦å¿«ï¼‰ã€‚
        """
        # 1. åŸºç¡€å¤„ç†ï¼šä»»ä½•æµé‡ä¸èƒ½ä¸ºè´Ÿ
        # è¿™é‡Œçš„ U è¿˜æ˜¯ (M, K)
        U_clamped = torch.clamp(U, min=0.0)

        # 2. è®¡ç®—æ¯æ¡è¾¹çš„æ€»æµé‡
        row_sum = U_clamped.sum(dim=1)

        # 3. æ‰¾å‡ºâ€œåè¾¹â€ï¼ˆActive Constraintsï¼‰ï¼šæ€»æµé‡è¶…è¿‡å®¹é‡ c çš„è¾¹
        # c æ˜¯ (M,)
        mask = row_sum > c

        # --- å¿«é€Ÿè·¯å¾„ ---
        # å¦‚æœæ²¡æœ‰ä»»ä½•è¾¹æ‹¥å µï¼Œç›´æ¥è¿”å›ï¼ˆè¿™åœ¨è¿­ä»£åˆæœŸéå¸¸å¸¸è§ï¼‰
        if not mask.any():
            return U_clamped.reshape(self.M * self.K)

        # --- æ…¢é€Ÿè·¯å¾„ï¼ˆåªé’ˆå¯¹åè¾¹ï¼‰ ---
        # æå–åè¾¹çš„å®¹é‡å’Œæµé‡
        # c_active: (N_active, )
        # U_active: (N_active, K)
        c_active = c[mask]
        U_active = U_clamped[mask]

        # *** æ ¸å¿ƒä¼˜åŒ–ï¼šåªå¯¹ mask é€‰å‡ºæ¥çš„è¿™éƒ¨åˆ†æ•°æ®è¿›è¡Œ Sort ***
        # è¿™é‡Œçš„è®¡ç®—é‡ä» M * K log K é™åˆ°äº† N_active * K log K
        U_sorted, _ = torch.sort(U_active, dim=1, descending=True)

        # --- ä¸‹é¢æ˜¯æ ‡å‡†çš„å•çº¯å½¢æŠ•å½±é€»è¾‘ (Simplex Projection) ---

        # è®¡ç®—å‰ç¼€å’Œ cumsum
        cssv = torch.cumsum(U_sorted, dim=1) - c_active.unsqueeze(1)

        # ç”Ÿæˆå¯¹åº”çš„é™¤æ•° range: [1, 2, 3, ..., K]
        # æ³¨æ„ï¼šä¸ºäº†é˜²æ­¢ shape å¹¿æ’­é”™è¯¯ï¼Œä¸€å®šè¦æ³¨æ„ç»´åº¦
        arange_k = torch.arange(1, self.K + 1, device=U.device, dtype=U.dtype)

        # è®¡ç®— rho çš„å€™é€‰å€¼
        cond = U_sorted - cssv / arange_k > 0.0

        # æ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„æœ€å¤§ç´¢å¼• rho
        # cond æ˜¯ boolï¼Œè½¬ float ç®— sum å¾—åˆ°æ»¡è¶³æ¡ä»¶çš„ä¸ªæ•°ï¼Œå‡ 1 å¾—åˆ°ä¸‹æ ‡
        rho = cond.sum(dim=1, keepdim=True).long() - 1

        # è·å–å¯¹åº”çš„ theta (é˜ˆå€¼)
        # gather ç”¨äºä» active çš„ cssv ä¸­å–å¯¹åº” rho ä½ç½®çš„å€¼
        theta = torch.gather(cssv, 1, rho) / (rho.float() + 1)

        # æ‰§è¡ŒæŠ•å½±ï¼šw = max(u - theta, 0)
        w_active = torch.clamp(U_active - theta, min=0.0)

        # --- ç»“æœå†™å› ---
        # å°†è®¡ç®—å¥½çš„æ‹¥å µè¾¹æŠ•å½±ç»“æœå†™å›å¤§çŸ©é˜µ
        # Clone æ˜¯ä¸ºäº†ä¸ç ´ååŸè®¡ç®—å›¾çš„ Leafï¼ˆè§†å…·ä½“ Autograd éœ€æ±‚ï¼Œä½†åœ¨ PDHG è¿™ç§ volatile ä¸Šä¸‹æ–‡ä¸­é€šå¸¸æ˜¯å®‰å…¨çš„ï¼‰
        U_out = U_clamped.clone()
        U_out[mask] = w_active

        return U_out.reshape(self.M * self.K)

    def make_initials(self, warm_start=True):
        # 0. åŸºç¡€å…¨é›¶åˆå§‹åŒ– (ä»¥é˜² W_adj æœªå‡†å¤‡å¥½)
        if self.W_adj is None and warm_start:
            print("Warning: W_adj is None, falling back to cold start.")
            return self._generate_cold_start()

        # 1. æ ¹æ®æ¨¡å¼åˆ†å‘
        if warm_start:
            # return self._generate_warm_start()
            return self._generate_mwu_warm_start()
        else:
            return self._generate_cold_start()

    def _generate_warm_start(self):
        """
        æœ€æ–°çš„ Warm Start ç­–ç•¥ï¼š
        x, X: Greedy Capacity Filling (è€ƒè™‘å®¹é‡é™åˆ¶)
        Y: Cost-to-Go Potential (æœ€çŸ­è·¯è·ç¦»)
        """
        device = self.device
        dtype = self.dtype
        K, M, N = self.K, self.M, self.N
        k_src, k_dst = self.k_src, self.k_dst
        demands, capacities = self.d, self.c
        weights = self.W

        # --- APSP ---
        # D, P, _ = ws.apsp_gpu(self.W_adj, dtype=dtype)
        D, P, _ = ws.parallel_bellman_ford_gpu(self.W_adj, dtype=dtype) # ä¼˜åŒ–è¿‡çš„ apsp_gpuï¼Œä¹‹åéƒ½é‡‡ç”¨è¿™ä¸ª
        del self.W_adj
        self.W_adj = None

        # --- è·¯å¾„è¿½è¸ª ---
        edge_lookup = torch.full((N, N), -1, dtype=torch.long, device=device)
        edge_lookup[self.edges_src, self.edges_dst] = torch.arange(M, device=device)

        x_flow = torch.zeros((M, K), dtype=dtype, device=device)
        curr_nodes = k_src.clone()
        active_mask = torch.ones(K, dtype=torch.bool, device=device)

        for _ in range(N):
            arrived = curr_nodes == k_dst
            active_mask = active_mask & (~arrived)
            if not active_mask.any():
                break

            next_nodes = P[curr_nodes, k_dst]
            edge_ids = edge_lookup[curr_nodes, next_nodes]
            valid_step = active_mask & (edge_ids != -1)
            
            if not valid_step.any():
                break

            valid_k = torch.nonzero(valid_step, as_tuple=True)[0]
            valid_e = edge_ids[valid_step]
            x_flow[valid_e, valid_k] = demands[valid_step]
            curr_nodes[valid_step] = next_nodes[valid_step]

        # --- Greedy Capacity Filling ---
        sorted_indices = torch.argsort(weights, descending=True)
        inv_sorted_indices = torch.argsort(sorted_indices)

        x_sorted = x_flow[:, sorted_indices]
        d_sorted = demands[sorted_indices]

        cum_flow = torch.cumsum(x_sorted, dim=1)
        prev_flow = cum_flow - x_sorted
        
        caps_expanded = capacities.view(-1, 1)
        residual_caps = (caps_expanded - prev_flow).clamp(min=0)
        allowed_flow_edge = torch.min(x_sorted, residual_caps)

        epsilon = 1e-6
        ratios_edge = allowed_flow_edge / (x_sorted + epsilon)
        path_mask = x_sorted > epsilon
        ratios_edge[~path_mask] = 1.0

        path_bottleneck_ratios, _ = torch.min(ratios_edge, dim=0)

        final_ratios = path_bottleneck_ratios[inv_sorted_indices]
        x_final = x_flow * final_ratios.view(1, -1)
        X_final = demands * final_ratios

        # --- Dual Y (Distance) ---
        Y_matrix = D[:, k_dst]
        finite_mask = torch.isfinite(Y_matrix)
        if finite_mask.any():
            max_val = Y_matrix[finite_mask].max()
            Y_matrix = torch.where(finite_mask, Y_matrix, max_val * 2.0)
        else:
            Y_matrix = torch.zeros_like(Y_matrix)

        return x_final.reshape(-1)*0.8, X_final*0.8, Y_matrix.reshape(-1)

    def _generate_cold_start(self):
        """
        æ—§çš„ Cold Start ç­–ç•¥ (å¤åˆ»)ï¼š
        å¯¹åº”åŸä»£ç ä¸­ x0=None, X0=None çš„æƒ…å†µã€‚
        ä¸è¿è¡Œ APSPï¼Œä¸è¿›è¡Œè·¯å¾„è¿½è¸ªã€‚
        
        é€»è¾‘ï¼š
        x0 = 0
        X0 = 0
        Y0 = -kappa * (A(x0) - S(X0)) => 0
        """
        device = self.device
        dtype = self.dtype
        
        # 1. åˆå§‹åŒ– x0 (å…¨0)
        x0 = torch.zeros(self.M * self.K, device=device, dtype=dtype)
        
        # 2. åˆå§‹åŒ– X0 (å…¨0)
        X0 = torch.zeros(self.K, device=device, dtype=dtype)
        
        # 3. åˆå§‹åŒ– Y0
        # å¯¹åº”æ—§ä»£ç  pdhg_solve ä¸­çš„: rY = self.A_matvec(x) - self.S_matvec(X)
        # å› ä¸º x=0, X=0ï¼Œæ‰€ä»¥ rY=0ï¼Œè¿›è€Œ Y=0ã€‚
        # Y çš„ç»´åº¦æ˜¯èŠ‚ç‚¹æ•° N * å•†å“æ•° K
        Y0 = torch.zeros(self.N * self.K, device=device, dtype=dtype)

        return x0, X0, Y0

    # æ— è®ºå¦‚ä½•ä¿®æ”¹ batchesï¼Œpdhgçš„è¿­ä»£è½®æ¬¡å§‹ç»ˆä¸å˜ï¼Œåº”è¯¥æ˜¯åœ¨ä¹‹å‰çš„ä»£ç ä¸­å­˜åœ¨é—®é¢˜ï¼ˆè®¨è®ºè§£å†³ï¼‰
    def _generate_mwu_warm_start(self, batches=1):
        """
        åŸºäº MWU (æ‹¥å¡æ„ŸçŸ¥è·¯ç”±) çš„ Warm Start ç­–ç•¥ã€‚
        
        åŸç†:
        å°†æ€»éœ€æ±‚åˆ†ä¸º `batches` ä»½ã€‚æ¯è½®è¿­ä»£è·¯ç”±ä¸€ä»½æµé‡ï¼Œ
        ç„¶åæ ¹æ®å½“å‰é“¾è·¯çš„æ‹¥å¡ç¨‹åº¦æé«˜è¾¹æƒï¼ˆæƒ©ç½šï¼‰ã€‚
        è¿™æ ·è¿«ä½¿åç»­æµé‡å¯»æ‰¾æ¬¡ä¼˜è·¯å¾„ï¼Œå½¢æˆè´Ÿè½½å‡è¡¡çš„åˆå§‹æµã€‚
        """
        device = self.device
        dtype = self.dtype
        K, M, N = self.K, self.M, self.N
        
        # åŸºç¡€æ•°æ®
        k_src, k_dst = self.k_src, self.k_dst
        demands, capacities = self.d, self.c
        base_cost = self.p # åŸºç¡€è¾¹è´¹ç”¨ (M,)
        
        # 1. åˆå§‹åŒ–ç´¯åŠ å™¨
        # x_total: ç´¯è®¡è¾¹æµé‡ (M, K)
        x_total = torch.zeros((M, K), dtype=dtype, device=device)
        # current_edge_weights: å½“å‰è¾¹æƒé‡ (M,)ï¼Œåˆå§‹ä¸ºåŸºç¡€è´¹ç”¨
        current_edge_weights = base_cost.clone()
        
        # æ¯è½®å¾…åˆ†é…çš„éœ€æ±‚é‡ (d / batches)
        chunk_demands = demands / batches

        # é¢„å…ˆæ„å»º edge lookup (ç”¨äºè·¯å¾„åæŸ¥ edge_id)
        # è¿™æ˜¯ä¸€ä¸ªå¸¸é‡è¡¨ï¼Œåªç”¨æ„å»ºä¸€æ¬¡
        edge_lookup = torch.full((N, N), -1, dtype=torch.long, device=device)
        edge_lookup[self.edges_src, self.edges_dst] = torch.arange(M, device=device)

        # ç”¨äº APSP çš„é‚»æ¥çŸ©é˜µ buffer (N, N)
        # åˆå§‹åŒ–ä¸ºæ— ç©·å¤§
        adj_matrix = torch.full((N, N), float('inf'), dtype=dtype, device=device)
        # å¯¹è§’çº¿ç½®0 (è™½ç„¶ APSP å¯èƒ½å¤„ç†ï¼Œä½†æ˜¾å¼å¤„ç†æ›´å®‰å…¨)
        adj_matrix.fill_diagonal_(0.0)

        # è®°å½•æœ€åä¸€æ¬¡çš„è·ç¦»çŸ©é˜µç”¨äº Dual åˆå§‹åŒ–
        last_D = None

        # é‡Šæ”¾åŸå§‹ W_adj èŠ‚çœæ˜¾å­˜ (æˆ‘ä»¬å°†åœ¨å¾ªç¯ä¸­åŠ¨æ€é‡å»ºå®ƒ)
        if self.W_adj is not None:
            del self.W_adj
            self.W_adj = None

        # --- MWU è¿­ä»£å¾ªç¯ ---
        for b in range(batches):
            # A. é‡å»ºé‚»æ¥çŸ©é˜µ (å¡«å…¥åŠ¨æ€æƒé‡)
            # é‡ç½®ä¸º inf
            adj_matrix.fill_(float('inf'))
            adj_matrix.fill_diagonal_(0.0)
            # å¡«å…¥å½“å‰è¾¹æƒ
            adj_matrix[self.edges_src, self.edges_dst] = current_edge_weights

            # B. è¿è¡Œ APSP (å¹¶è¡Œ Bellman-Ford æˆ– Floyd)
            # D: (N, N), P: (N, N)
            D, P, _ = ws.parallel_bellman_ford_gpu(adj_matrix, dtype=dtype)
            last_D = D

            # C. è·¯å¾„è¿½è¸ª & æœ¬è½®æµé‡åˆ†é…
            # (é€»è¾‘å¤ç”¨ generate_initial_flow_gpuï¼Œä½†é’ˆå¯¹ chunk_demands)
            x_batch = torch.zeros((M, K), dtype=dtype, device=device)
            curr_nodes = k_src.clone()
            active_mask = torch.ones(K, dtype=torch.bool, device=device)

            for _ in range(N): # æœ€é•¿è·¯å¾„ N
                arrived = (curr_nodes == k_dst)
                active_mask = active_mask & (~arrived)
                if not active_mask.any():
                    break
                
                # æŸ¥ä¸‹ä¸€è·³
                next_nodes = P[curr_nodes, k_dst]
                edge_ids = edge_lookup[curr_nodes, next_nodes]
                valid_step = active_mask & (edge_ids != -1)
                
                if not valid_step.any():
                    break
                
                valid_k = torch.nonzero(valid_step, as_tuple=True)[0]
                valid_e = edge_ids[valid_step]
                
                # ç´¯åŠ æœ¬è½®æµé‡
                x_batch[valid_e, valid_k] = chunk_demands[valid_step]
                curr_nodes[valid_step] = next_nodes[valid_step]

            # D. ç´¯åŠ åˆ°æ€»æµé‡
            x_total += x_batch

            # E. æ‹¥å¡æ„ŸçŸ¥ï¼šæ›´æ–°è¾¹æƒé‡ (ä¸ºä¸‹ä¸€è½®åšå‡†å¤‡)
            if b < batches - 1: # æœ€åä¸€è½®ä¸éœ€è¦æ›´æ–°
                # è®¡ç®—å½“å‰æ¯æ¡è¾¹çš„æ€»æµé‡
                edge_flow_sum = x_total.sum(dim=1) # (M,)
                
                # è®¡ç®—æ‹¥å¡æ¯”ç‡ (Flow / Capacity)
                # åŠ ä¸€ä¸ªå° epsilon é˜²æ­¢é™¤é›¶
                congestion = edge_flow_sum / (capacities + 1e-6)
                
                # æ ¸å¿ƒ MWU æƒ©ç½šå…¬å¼:
                # ç­–ç•¥: åŸºç¡€è´¹ç”¨ * (1 + alpha * æ‹¥å¡åº¦^2)
                # ä½¿ç”¨å¹³æ–¹é¡¹æ˜¯ä¸ºäº†è®©æ¥è¿‘å®¹é‡è¾¹ç•Œçš„æƒ©ç½šæ€¥å‰§å¢åŠ 
                penalty_factor = 1.0 + 5.0 * torch.pow(congestion, 2)
                
                current_edge_weights = base_cost * penalty_factor
                
                # å¿…é¡»ä¿è¯æƒé‡ä¸ä¸ºè´Ÿä¸”ä¸è¿‡å° (Bellman-Ford ç¨³å®šæ€§)
                current_edge_weights = torch.max(current_edge_weights, base_cost)


        # --- ç»“æœç»„è£… ---
        
        # 1. åŸå§‹æµ x
        # åŠ ä¸Šç³»æ•° 0.9ï¼Œç¨å¾®ç•™ä¸€ç‚¹ä½™é‡ï¼Œé˜²æ­¢åˆå§‹ç‚¹ç›´æ¥ä½äºå¯è¡ŒåŸŸè¾¹ç•Œä¸Šå¯¼è‡´æ¢¯åº¦éœ‡è¡
        x_final = x_total * 0.9
        
        # 2. å·²é€è¾¾é‡ X
        # MWU å‡è®¾å…¨é¢å‘é€ï¼Œæ‰€ä»¥ X = demands * 0.9
        X_final = demands * 0.9

        # 3. å¯¹å¶å˜é‡ Y (Distance / Potential)
        # ä½¿ç”¨æœ€åä¸€æ¬¡è¿­ä»£çš„ D (æ­¤æ—¶åŒ…å«æ‹¥å¡æƒ©ç½šï¼Œæœ€èƒ½åæ˜ çœŸå®è·¯å†µ)
        # Y[u, k] åº”è¿‘ä¼¼äº dist(u, k_dst)
        # D shape (N, N), D[:, dst] å–å‡ºçš„æ˜¯ "to dst" çš„è·ç¦»åˆ—å‘é‡
        Y_matrix = last_D[:, k_dst]  # type: ignore
        
        # å¤„ç†æ— ç©·å¤§ (ä¸å¯è¾¾çš„æƒ…å†µ)
        finite_mask = torch.isfinite(Y_matrix)
        if finite_mask.any():
            max_val = Y_matrix[finite_mask].max()
            # å°†ä¸å¯è¾¾ç‚¹çš„åŠ¿èƒ½è®¾ä¸ºæœ€å¤§å€¼çš„ä¸¤å€ï¼Œç»™ä¸€ä¸ªæ¢¯åº¦æŒ‡å¼•
            Y_matrix = torch.where(finite_mask, Y_matrix, max_val * 2.0)
        else:
            Y_matrix = torch.zeros_like(Y_matrix)
        
        # å°†åŠ¿èƒ½æ–¹å‘åè½¬ï¼Ÿ 
        # PDHGå®šä¹‰ AT_matvec = pot[dst] - pot[src]
        # æœ€çŸ­è·¯æ€§è´¨: dist[dst] - dist[src] <= weight
        # é€šå¸¸ Y å– dist æ˜¯åˆç†çš„
        
        return x_final.reshape(-1), X_final, Y_matrix.reshape(-1)
